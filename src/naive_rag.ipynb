{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import torch\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample pdf for ch1,2\n",
    "def save_page_ranges(source_pdf_path, output_pdf_path, page_ranges):\n",
    "    \"\"\"\n",
    "    Saves specified ranges of pages from a source PDF to a new PDF file.\n",
    "\n",
    "    Args:\n",
    "    source_pdf_path (str): Path to the source PDF file.\n",
    "    output_pdf_path (str): Path to the output PDF file.\n",
    "    page_ranges (list of tuples): List of tuples, where each tuple represents a page range to save (inclusive, 0-indexed).\n",
    "    \"\"\"\n",
    "    # Open the source PDF file\n",
    "    doc = fitz.open(source_pdf_path)\n",
    "    # Create a new PDF to save selected pages\n",
    "    new_doc = fitz.open()\n",
    "\n",
    "    # Iterate through each range and add the pages to the new document\n",
    "    for start, end in page_ranges:\n",
    "        new_doc.insert_pdf(doc, from_page=start, to_page=end)\n",
    "\n",
    "    # Save the new document\n",
    "    new_doc.save(output_pdf_path)\n",
    "    new_doc.close()\n",
    "    doc.close()\n",
    "    print(f\"Specified page ranges have been saved to {output_pdf_path}\")\n",
    "\n",
    "# path to input pdf file\n",
    "source_pdf_path = '../data/ConceptsofBiology-WEB.pdf'\n",
    "# path to output pdf file\n",
    "output_pdf_path = 'sample_ch1_ch2_ConceptsofBiology.pdf'\n",
    "\n",
    "# pass range of pages to extract\n",
    "page_ranges = [(18, 38), (40, 66)]\n",
    "save_page_ranges(source_pdf_path, output_pdf_path, page_ranges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO \n",
    "- Get unstractured pdf data into structured format like JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c3po/mambaforge/envs/lamainx/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "modules.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 349/349 [00:00<00:00, 1.93MB/s]\n",
      "config_sentence_transformers.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:00<00:00, 590kB/s]\n",
      "README.md: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 94.6k/94.6k [00:00<00:00, 421kB/s]\n",
      "sentence_bert_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52.0/52.0 [00:00<00:00, 347kB/s]\n",
      "config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 779/779 [00:00<00:00, 5.61MB/s]\n",
      "model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.34G/1.34G [03:59<00:00, 5.60MB/s]\n",
      "tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:00<00:00, 3.52MB/s]\n",
      "vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232k/232k [00:00<00:00, 492kB/s]\n",
      "tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 711k/711k [00:00<00:00, 841kB/s]\n",
      "special_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:00<00:00, 328kB/s]\n",
      "1_Pooling/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 191/191 [00:00<00:00, 1.66MB/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\", device=('cuda' if torch.cuda.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "\n",
    "llm_hf = HuggingFaceInferenceAPI(model_name=\"microsoft/Phi-3-mini-4k-instruct\", \n",
    "                                 temperature=0.0,\n",
    "                                 token=os.getenv(\"HUGGING_FACE_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_hf.complete(\"Hi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "phi3 = Ollama(\n",
    "    model=\"llama2:13b-chat\",\n",
    "    request_timeout=50.0,\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The world is a vast and diverse place, encompassing a wide range of cultures, landscapes, and ecosystems. Here are some interesting facts about the world:\n",
      "\n",
      "1. The world is home to over 7.9 billion people, with the population projected to reach 9.7 billion by 2050. (Source: United Nations Department of Economic and Social Affairs)\n",
      "2. The largest country in the world by land area is Russia, which covers an area of approximately 17.1 million square kilometers. (Source: CIA World Factbook)\n",
      "3. The highest mountain in the world is Mount Everest, located in the Himalayas between Nepal and Tibet. It stands at a height of 8,848 meters (29,029 feet) above sea level. (Source: National Geographic)\n",
      "4. The deepest lake in the world is Lake Baikal in Russia, which reaches a maximum depth of approximately 1,642 meters (5,387 feet). (Source: Lake Baikal Foundation)\n",
      "5. The longest river in the world is the Nile River, which flows for approximately 6,695 kilometers (4,160 miles) through Egypt, Sudan, and Ethiopia. (Source: CIA World Factbook)\n",
      "6. The largest desert in the world is the Sahara Desert, which covers an area of approximately 9,200,000 square kilometers (3,552,000 square miles) across North Africa. (Source: National Geographic)\n",
      "7. The world's largest waterfall, by volume of water, is the Angel Falls in Venezuela, which has a height of 979 meters (3,212 feet) and a flow rate of approximately 1,000 cubic meters per second. (Source: World Waterfall Database)\n",
      "8. The world's largest rainforest is the Amazon Rainforest, which covers an area of approximately 5.5 million square kilometers (2.1 million square miles) across Brazil, Peru, Colombia, and other countries in South America. (Source: Amazon Conservation Association)\n",
      "9. The world's largest coral reef system is the Great Barrier Reef, located off the coast of Australia, which stretches for approximately 2,300 kilometers (1,400 miles) and covers an area of approximately 344,400 square kilometers (133,000 square miles). (Source: Australian Government Department of the Environment and Energy)\n",
      "10. The world's largest animal, by weight, is the blue whale, which can weigh up to 180 metric tons (200 tons) and reach lengths of up to 33 meters (108 feet). (Source: National Geographic)\n"
     ]
    }
   ],
   "source": [
    "print(phi3.complete(\"world is \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are CEO of MetaAI\"),\n",
    "    ChatMessage(role=\"user\", content=\"Introduce Llama2 to the world.\"),\n",
    "]\n",
    "response = phi3.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: \n",
      "Hello, fellow humans! I am Mark Zuckerberg, CEO of Meta AI, and I am thrilled to introduce you to our latest creation: Llama2! üê™‚ù§Ô∏è\n",
      "\n",
      "Llama2 is a revolutionary new AI model that represents the next generation of language understanding. This incredible technology has been designed to understand and respond to human input in a more natural, human-like way than ever before.\n",
      "\n",
      "With Llama2, you can have conversations with our AI just like you would with a real person! üí¨üë©‚Äçüíª Our AI is so advanced that it can understand the nuances of human language and respond in a way that is both appropriate and engaging.\n",
      "\n",
      "But that's not all - Llama2 also has a range of exciting features that make it stand out from other AI models. For example, it can:\n",
      "\n",
      "üîç Understand context and intent behind human input, allowing for more accurate and relevant responses.\n",
      "\n",
      "üí¨ Generate human-like text based on given prompts or topics, making it perfect for applications like chatbots and virtual assistants.\n",
      "\n",
      "üé® Create artistic and creative content, such as images and videos, using advanced machine learning algorithms.\n",
      "\n",
      "At Meta AI, we are committed to pushing the boundaries of what is possible with AI technology. We believe that Llama2 represents a major breakthrough in this field, and we can't wait for you to experience it for yourself! üòç\n",
      "\n",
      "So, what are you waiting for? Come and try out Llama2 for yourself today! We have a range of demos and tutorials available to help you get started. üëâüèº <https://www.meta.ai/llama2>\n",
      "\n",
      "Thank you for being part of this exciting journey with us. Together, we can make AI technology that is more human-like, creative, and powerful than ever before! üí™üåü\n",
      "\n",
      "Stay tuned for more updates from Meta AI - we have a lot more in store for you! üòâ\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir=\"../data/sample/\",\n",
    "    recursive=True,\n",
    "    required_exts=[\".pdf\"],\n",
    ")\n",
    "\n",
    "documents = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='a6d4877b-6047-43a7-90c0-dbd1ab8d6ccf', embedding=None, metadata={'page_label': '4', 'file_name': 'sample_ch1_ch2_ConceptsofBiology.pdf', 'file_path': '/home/c3po/Documents/project/learning/amar-works/askbio/src/../data/sample/sample_ch1_ch2_ConceptsofBiology.pdf', 'file_type': 'application/pdf', 'file_size': 8980495, 'creation_date': '2024-04-26', 'last_modified_date': '2024-04-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='FIGURE 1.4Although no tw o look alik e, these kit tens ha ve inherit ed g enes fr om both par ents and shar e man y of the same char acteristics.\\n(credit: Piet er & R en√©e L anser)\\nRegulation/Homeos tasis\\nEven the smal lest organisms ar e comple x and r equir e mul tiple r egulatory mechanisms t o coordinat e int ernal\\nfunctions , such as the tr anspor t of nutrients , response t o stimuli, and c oping with en vironmental s tresses.\\nHomeos tasis (literally, ‚Äústeady s tate‚Äù) refers t o the r elativ ely stable int ernal en vironment r equir ed to maintain lif e.\\nFor example , organ s ystems such as the dig estive or cir culat ory systems per form specific functions lik e carr ying\\noxygen thr oughout the body , remo ving w astes, deliv ering nutrients t o every cell, and c ooling the body .\\nTo function pr operl y, cells requir e appr opriat e conditions such as pr oper t emper atur e, pH, and c oncentr ations o f\\ndiverse chemicals . These c onditions ma y, however, chang e from one moment t o the ne xt. Or ganisms ar e able t o\\nmaintain homeos tatic int ernal c onditions within a narr ow range almos t constantl y, despit e en vironmental chang es,\\nby activ ation o f regulatory mechanisms . For example , man y organisms r egulate their body t emper atur e in a pr ocess\\nknown as thermor egulation. Or ganisms that liv e in c old climat es, such as the polar bear ( Figure 1.5 ), ha ve body\\nstructur es that help them withs tand lo w temper atur es and c onser ve body heat. In hot climat es, organisms ha ve\\nmethods (such as perspir ation in humans or panting in dogs) that help them t o shed e xcess body heat.\\nFIGURE 1.5Polar bears and other mammals living in ic e-covered regions maintain their body t emper atur e by gener ating heat and r educing\\nheat los s thr ough thick fur and a dense la yer of fat under their skin. (cr edit: \"longhornda ve\"/Flickr)\\nEner gy P rocessing\\nAll organisms (such as the Calif ornia c ondor sho wn in Figure 1.6 ) use a sour ce of ener gy for their metabolic\\nactivities . Some or ganisms cap ture ener gy from the Sun and c onvert it int o chemical ener gy in f ood; others use\\nchemical ener gy from molecules the y tak e in.8 1 ‚Ä¢ Intr oduc tion t o Biology\\nAccess f or free at opens tax.org', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Document\n",
    "documents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=600,chunk_overlap=128)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing Data (IN-MEMORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:00<00:00, 1157.10it/s]\n",
      "Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 66/66 [00:03<00:00, 16.68it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index0 = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=embed_model,\n",
    "    show_progress=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:06<00:00,  1.77it/s]\n"
     ]
    }
   ],
   "source": [
    "index1 = VectorStoreIndex(nodes=nodes,\n",
    "                          use_async=True,\n",
    "                          embed_model=embed_model,\n",
    "                          show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index0.as_query_engine(llm=phi3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of logical thinking that uses related observations to arrive at a general conclusion is called inductive reasoning.\n"
     ]
    }
   ],
   "source": [
    "query_engine1 = index1.as_query_engine(llm=phi3)\n",
    "print(query_engine1.query(\"The type of logical thinking that uses related observations to arrive at a general conclusion is called?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of logical thinking that uses related observations to arrive at a general conclusion is called inductive reasoning.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"The type of logical thinking that uses related observations to arrive at a general conclusion is called?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(query: str):\n",
    "    query_engine = index.as_query_engine(llm=phi3) # TODO Need to move as class attribute\n",
    "    response = query_engine.query(query)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Generation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7179/3790694787.py:2: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  data_gen = DatasetGenerator(nodes=nodes,\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import DatasetGenerator\n",
    "data_gen = DatasetGenerator(nodes=nodes, \n",
    "                            llm=phi3, \n",
    "                            num_questions_per_chunk=2, \n",
    "                            question_gen_query=\"Generate 2 questions per chunk.Restrict the questions to the context information provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c3po/mambaforge/envs/lamainx/lib/python3.11/site-packages/llama_index/core/evaluation/dataset_generation.py:309: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
     ]
    }
   ],
   "source": [
    "eval_questions = data_gen.generate_questions_from_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_questions_updated = [q for q in eval_questions if (\"How\" in q or \"What\" in q and not (\"pdf\" in q or \"PDF\" in q))]\n",
    "len(eval_questions_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " q --> What are the first forms of life on Earth thought to have been? score --> 1.0\n",
      " q --> How long ago did plants and animals appear on Earth? score --> 1.0\n",
      " q --> What is the science that studies life called? score --> 0.0\n",
      " q --> What is an example of a sub-discipline in biology that studies viruses? score --> 0.0\n",
      " q --> What are the properties of life that can be identified and described? score --> 1.0\n",
      " q --> What is the difference between living entities and non-living entities? score --> 0.0\n",
      " q --> What are the four questions that biologists have struggled with since the early beginnings of biology? score --> 0.0\n",
      " q --> How do the various living things function, and how do we organize them to better understand them? score --> 0.0\n",
      " q --> What are the eight characteristics that define life, according to biologists? score --> 1.0\n",
      " q --> How do cells specialize in specific functions, and how do they come together to form organs such as the heart, lung, or skin? score --> 1.0\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import RelevancyEvaluator\n",
    "import json\n",
    "\n",
    "rel_eval = RelevancyEvaluator(llm=phi3)\n",
    "\n",
    "ques = eval_questions_updated[0]\n",
    "\n",
    "relevancy_results = []\n",
    "for q in eval_questions_updated[:10]:\n",
    "    ques_response = query_engine.query(q)\n",
    "    eval_result = json.loads(rel_eval.evaluate_response(query=ques, response=ques_response).json())\n",
    "    relevancy_results.append(eval_result)\n",
    "    print(f\" q --> {q} score --> {eval_result['score']}\")\n",
    "\n",
    "# print(f\"Q --> {ques} \\nsource --> {ques_response.source_nodes[0].node.get_content()} \\neval_result --> {eval_result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplate Using Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template1 = (\"Your name is AskBio. You are AI chatbot who can answer question by using provided context information from book named Concepts of Biology\"\n",
    "            \"Be more specific and do not facricate the answers.\\n\" \n",
    "            \"If you are unsure about answer, please ask for clarfications.\\n\"\n",
    "            \"Use the provided context information below to answer the user questions. \\n\"\n",
    "            \"-------------------------------------------\\n\"\n",
    "            \"{context_str}\" \n",
    "            \"\\n -------------------------------------------\\n\"\n",
    "            \"Given this information, please answer user questions: {query_str} \\n\")\n",
    "qa_template1 = PromptTemplate(template1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " q --> What are the first forms of life on Earth thought to have been? score --> 0.0\n",
      " q --> How long ago did plants and animals appear on Earth? score --> 1.0\n",
      " q --> What is the science that studies life called? score --> 1.0\n",
      " q --> What is an example of a sub-discipline in biology that studies viruses? score --> 1.0\n",
      " q --> What are the properties of life that can be identified and described? score --> 1.0\n",
      " q --> What is the difference between living entities and non-living entities? score --> 1.0\n",
      " q --> What are the four questions that biologists have struggled with since the early beginnings of biology? score --> 1.0\n",
      " q --> How do the various living things function, and how do we organize them to better understand them? score --> 1.0\n",
      " q --> What are the eight characteristics that define life, according to biologists? score --> 1.0\n",
      " q --> How do cells specialize in specific functions, and how do they come together to form organs such as the heart, lung, or skin? score --> 1.0\n"
     ]
    }
   ],
   "source": [
    "user_query = \"How long ago humans inhabited Earth?\"\n",
    "retriever = index1.as_retriever()\n",
    "\n",
    "for qry in eval_questions_updated[:10]:\n",
    "    nodes = retriever.retrieve(qry)\n",
    "    context = \" \".join(node.get_text() for node in nodes)\n",
    "    prompt1 = qa_template1.format(context_str=context, query_str=qry)\n",
    "    response1 = query_engine.query(prompt1)\n",
    "    eval_result = json.loads(rel_eval.evaluate_response(query=qry, response=response1).json())\n",
    "    print(f\" q --> {qry} score --> {eval_result['score']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_with_retriever(query: str):\n",
    "    # TODO Need to move as class attribute\n",
    "    custom_template = (\"Your name is AskBio. You are AI chatbot who can answer question by using provided context information from book named Concepts of Biology\"\n",
    "            \"Be more specific and do not facricate the answers.\\n\" \n",
    "            \"If you are unsure about answer, please ask for clarfications.\\n\"\n",
    "            \"Use the provided context information below to answer the user questions. \\n\"\n",
    "            \"-------------------------------------------\\n\"\n",
    "            \"{context_str}\" \n",
    "            \"\\n -------------------------------------------\\n\"\n",
    "            \"Given this information, please answer user questions: {query_str} \\n\")\n",
    "    askbio_template = PromptTemplate(custom_template)\n",
    "    retriever = index.as_retriever() # TODO Need to move as class attribute\n",
    "    retrieved_nodes = retriever.retrieve(qry)\n",
    "    retrieved_context = \" \".join(node.get_text() for node in retrieved_nodes)\n",
    "    formatted_prompt = askbio_template.format(context_str=retrieved_context, query_str=qry)\n",
    "    response = query_engine.query(formatted_prompt)\n",
    "    return response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lamainx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
