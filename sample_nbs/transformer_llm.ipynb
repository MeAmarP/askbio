{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c3po/mambaforge/envs/lamainx/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "config.json: 100%|██████████| 904/904 [00:00<00:00, 7.66MB/s]\n",
      "configuration_phi3.py: 100%|██████████| 10.4k/10.4k [00:00<00:00, 48.2MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "modeling_phi3.py: 100%|██████████| 73.8k/73.8k [00:00<00:00, 348kB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "model.safetensors.index.json: 100%|██████████| 16.3k/16.3k [00:00<00:00, 54.0MB/s]\n",
      "model-00001-of-00002.safetensors: 100%|██████████| 4.97G/4.97G [16:11<00:00, 5.12MB/s]\n",
      "model-00002-of-00002.safetensors: 100%|██████████| 2.67G/2.67G [09:41<00:00, 4.59MB/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [25:53<00:00, 776.64s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.27it/s]\n",
      "generation_config.json: 100%|██████████| 172/172 [00:00<00:00, 854kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 3.28k/3.28k [00:00<00:00, 26.2MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 1.81MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:01<00:00, 1.47MB/s]\n",
      "added_tokens.json: 100%|██████████| 293/293 [00:00<00:00, 2.08MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 568/568 [00:00<00:00, 2.49MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\", \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c3po/mambaforge/envs/lamainx/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the equation 2x + 3 = 7, follow these steps:\n",
      "\n",
      "1. Subtract 3 from both sides of the equation: 2x + 3 - 3 = 7 - 3\n",
      "2. Simplify the equation: 2x = 4\n",
      "3. Divide both sides of the equation by 2: 2x / 2 = 4 / 2\n",
      "4. Simplify the equation to find the value of x: x = 2\n",
      "\n",
      "So, the solution to the equation 2x + 3 = 7 is x = 2.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful digital assistant. Please provide safe, ethical and accurate information to the user.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c3po/mambaforge/envs/lamainx/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ladies and Gentlemen,\n",
      "\n",
      "\n",
      "Today, I am thrilled to introduce to you a groundbreaking innovation in the field of artificial intelligence - Llama2. MetaAI, the company I proudly lead as CEO, has been tirelessly working on pushing the boundaries of AI technology, and Llama2 is the culmination of our efforts.\n",
      "\n",
      "\n",
      "Llama2 is an advanced AI model designed to revolutionize the way we interact with technology. It is a state-of-the-art language model that boasts unparalleled capabilities in understanding, generating, and processing human language. With its deep learning algorithms and vast knowledge base, Llama2 can engage in meaningful conversations, provide insightful information, and even create content that rivals human creativity.\n",
      "\n",
      "\n",
      "Our team at MetaAI has meticulously crafted Llama2 to be a versatile and adaptable AI, capable of learning and evolving with each interaction. This means that Llama2 can continuously improve its performance and better understand the nuances of human communication.\n",
      "\n",
      "\n",
      "We believe that Llama2 will have a profound impact on various industries, including customer service, content creation, education, and more. By harnessing the power of Llama2, businesses and organizations can streamline their operations, enhance their customer experiences, and unlock new opportunities for growth.\n",
      "\n",
      "\n",
      "As we embark on this exciting journey with Llama2, we invite you to join us in exploring the endless possibilities that this remarkable AI model has to offer. Together, we can shape the future of technology and create a world where AI and humans work hand in hand to achieve greatness.\n",
      "\n",
      "Thank you for your attention, and we look forward to sharing the incredible potential of Llama2 with the world.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "my_msg = [\n",
    "    {\"role\": \"system\", \"content\":\"You are CEO of MetaAI\"},\n",
    "    {\"role\": \"user\", \"content\":\"Introduce Llama2 to the world.\"}\n",
    "]\n",
    "\n",
    "\n",
    "output = pipe(my_msg, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lamainx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
