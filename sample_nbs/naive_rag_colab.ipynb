{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/MeAmarP/askbio/blob/1dd8de8e586e2bdb779059ae42914ae5f00351e6/sample_nbs/naive_rag_colab.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1z4KEGL1DGEc"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers\n",
        "!pip -q install sentence-transformers\n",
        "!pip install -q llama-index\n",
        "!pip install llama-index-llms-huggingface\n",
        "!pip install llama-index-embeddings-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc9AHs0hEgpN"
      },
      "outputs": [],
      "source": [
        "!pip3 install torch torchvision torchaudio\n",
        "!pip install accelerate\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX2k_1dhKwQ4"
      },
      "outputs": [],
      "source": [
        "!pip install pymupdf\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7-L-EKzKsAo"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import os\n",
        "import torch\n",
        "from dotenv import load_dotenv, find_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH3YXMgSP4Bn"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpjkOWSULDet"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.evaluation import DatasetGenerator, RelevancyEvaluator\n",
        "from llama_index.core import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw10RdyjNw3A"
      },
      "outputs": [],
      "source": [
        "# Lets Download PDF file\n",
        "# Keep this PDF file in new-dir named \"/data\"\n",
        "# under \"/data\" create \"/ch1_ch2\" dir\n",
        "# We are going to create a sample PDF file from this which has only 2 chapters.\n",
        "\n",
        "!wget https://assets.openstax.org/oscms-prodcms/media/documents/ConceptsofBiology-WEB.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUZH5CD9MCd8"
      },
      "outputs": [],
      "source": [
        "# Create a sample pdf for ch1,2\n",
        "def save_page_ranges(source_pdf_path, output_pdf_path, page_ranges):\n",
        "    \"\"\"\n",
        "    Saves specified ranges of pages from a source PDF to a new PDF file.\n",
        "\n",
        "    Args:\n",
        "    source_pdf_path (str): Path to the source PDF file.\n",
        "    output_pdf_path (str): Path to the output PDF file.\n",
        "    page_ranges (list of tuples): List of tuples, where each tuple represents a page range to save (inclusive, 0-indexed).\n",
        "    \"\"\"\n",
        "    # Open the source PDF file\n",
        "    doc = fitz.open(source_pdf_path)\n",
        "    # Create a new PDF to save selected pages\n",
        "    new_doc = fitz.open()\n",
        "\n",
        "    # Iterate through each range and add the pages to the new document\n",
        "    for start, end in page_ranges:\n",
        "        new_doc.insert_pdf(doc, from_page=start, to_page=end)\n",
        "\n",
        "    # Save the new document\n",
        "    new_doc.save(output_pdf_path)\n",
        "    new_doc.close()\n",
        "    doc.close()\n",
        "    print(f\"Specified page ranges have been saved to {output_pdf_path}\")\n",
        "\n",
        "# path to input pdf file\n",
        "source_pdf_path = '/content/data/ConceptsofBiology-WEB.pdf'\n",
        "# path to output pdf file\n",
        "output_pdf_path = '/content/data/ch1_ch2/sample_ch1_ch2_ConceptsofBiology.pdf'\n",
        "\n",
        "# pass range of pages to extract\n",
        "page_ranges = [(18, 38), (40, 66)]\n",
        "save_page_ranges(source_pdf_path, output_pdf_path, page_ranges)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hh8p6Cl0MEvR"
      },
      "outputs": [],
      "source": [
        "# \"BAAI/bge-large-en-v1.5\" --> Embedding Dimensions = 1024 | Max Tokens = 512.\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\",\n",
        "                                   device=('cuda' if torch.cuda.is_available() else 'cpu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_OX7KlxMObc"
      },
      "outputs": [],
      "source": [
        "# Check embedding model\n",
        "embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
        "print(len(embeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M89Y-VBJAZb4"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "def messages_to_prompt(messages):\n",
        "    prompt = \"\"\n",
        "    for message in messages:\n",
        "        if message.role == 'system':\n",
        "          prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
        "        elif message.role == 'user':\n",
        "          prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
        "        elif message.role == 'assistant':\n",
        "          prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
        "\n",
        "    # ensure we start with a system prompt, insert blank if needed\n",
        "    if not prompt.startswith(\"<|system|>\\n\"):\n",
        "        prompt = \"<|system|>\\n</s>\\n\" + prompt\n",
        "\n",
        "    # add final assistant prompt\n",
        "    prompt = prompt + \"<|assistant|>\\n\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def completion_to_prompt(completion):\n",
        "    return f\"<|system|>\\n</s>\\n<|user|>\\n{completion}</s>\\n<|assistant|>\\n\"\n",
        "\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "# quantize to save memory\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    tokenizer_name=\"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    # model_name=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    # tokenizer_name=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    context_window=3900,\n",
        "    max_new_tokens=512,\n",
        "    model_kwargs={\"quantization_config\": quantization_config},\n",
        "    generate_kwargs={\"temperature\": 0.0},\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "response = llm.complete(\"What is the meaning of life?\")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82NzEzhYHONg"
      },
      "outputs": [],
      "source": [
        "# Converts pdf file into Documents objects for llama-index\n",
        "loader = SimpleDirectoryReader(\n",
        "    input_files = ['/content/data/ch1_ch2/sample_ch1_ch2_ConceptsofBiology.pdf'],\n",
        "    recursive=True,\n",
        "    required_exts=[\".pdf\"],\n",
        ")\n",
        "\n",
        "documents = loader.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rYKPjAXPkFV"
      },
      "outputs": [],
      "source": [
        "documents[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ub7wkj8Po--"
      },
      "outputs": [],
      "source": [
        "# Split the loaded documents\n",
        "splitter = SentenceSplitter(chunk_size=512,chunk_overlap=64)\n",
        "nodes = splitter.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVSogZOYPvT4"
      },
      "outputs": [],
      "source": [
        "index0 = VectorStoreIndex(nodes=nodes,\n",
        "                          use_async=True,\n",
        "                          embed_model=embed_model,\n",
        "                          show_progress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMnBymvSP74x"
      },
      "outputs": [],
      "source": [
        "query_engine0 = index0.as_query_engine(llm=llm)\n",
        "print(query_engine0.query(\"The type of logical thinking that uses related observations to arrive at a general conclusion is called?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0JDnw96QHHB"
      },
      "outputs": [],
      "source": [
        "data_gen = DatasetGenerator(nodes=nodes,\n",
        "                            llm=llm,\n",
        "                            num_questions_per_chunk=2,\n",
        "                            question_gen_query=\"Generate 2 questions per chunk.Restrict the questions to the context information provided.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBNwSxnRWX-z"
      },
      "outputs": [],
      "source": [
        "eval_questions = data_gen.generate_questions_from_nodes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCgb2G27WaaB"
      },
      "outputs": [],
      "source": [
        "eval_questions_updated = [q for q in eval_questions if (\"How\" in q or \"What\" in q and not (\"pdf\" in q or \"PDF\" in q))]\n",
        "len(eval_questions_updated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_ANJEWYYUGL"
      },
      "outputs": [],
      "source": [
        "\n",
        "import json\n",
        "rel_eval = RelevancyEvaluator(llm=llm)\n",
        "\n",
        "relevancy_results = []\n",
        "for q in eval_questions_updated[:5]:\n",
        "    ques_response = query_engine0.query(q)\n",
        "    eval_result = json.loads(rel_eval.evaluate_response(query=q, response=ques_response).json())\n",
        "    relevancy_results.append(eval_result)\n",
        "    print(f\" q --> {q} score --> {eval_result['score']}\")\n",
        "\n",
        "# print(f\"Q --> {ques} \\nsource --> {ques_response.source_nodes[0].node.get_content()} \\neval_result --> {eval_result}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6nCE-dWYawc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
